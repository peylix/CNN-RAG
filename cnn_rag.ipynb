{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoSense: A Hybrid CNN-RAG Framework for Mental Health Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CNN-based Facial Emotion Recognition Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "from keras.models import load_model  \n",
    "import json  \n",
    "from datetime import datetime  \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'test_video.mp4'\n",
    "\n",
    "# Load exist model  \n",
    "model = load_model('model_keras.h5')  \n",
    "\n",
    "# Load video file  \n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain properties of the video and perform other preliminary works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Load Haar Cascade model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Create list to store results\n",
    "emotion_results = []\n",
    "json_filename = f'emotion_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "# Counter for controlling save frequency\n",
    "counter = 0\n",
    "save_interval = 10  # Save every 10 frames\n",
    "\n",
    "# Playback control\n",
    "paused = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the emotion detection process and save the data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if not paused:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video ended\")\n",
    "            break\n",
    "\n",
    "        # Get current frame number\n",
    "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        \n",
    "        # Calculate video progress\n",
    "        progress = (current_frame / frame_count) * 100\n",
    "\n",
    "        # Turn captured frame to grayscale images\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect human face\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            # Extract the face area and resize it\n",
    "            face = gray[y:y+h, x:x+w]\n",
    "            face = cv2.resize(face, (48, 48))\n",
    "\n",
    "            # Preprocess the image (normalize)\n",
    "            face = face / 255.0\n",
    "            face = np.stack((face,)*3, axis=-1)\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "\n",
    "            # Predicting emotions\n",
    "            emotion_prediction = model.predict(face)\n",
    "            emotion_label = np.argmax(emotion_prediction)\n",
    "            \n",
    "            names = ['anger','contempt','disgust','fear','happy','sadness','surprise','neutral']\n",
    "            current_emotion = names[emotion_label]\n",
    "\n",
    "            # Add logic to convert disgust to neutral with 90% probability\n",
    "            if current_emotion == 'disgust' and random.random() <= 0.9:\n",
    "                current_emotion = 'neutral'\n",
    "\n",
    "            # Display predicted sentiment on images\n",
    "            cv2.putText(frame, f'Emotion: {current_emotion}', (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "            # Save results every N frames\n",
    "            if counter % save_interval == 0:\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                frame_data = {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"frame_number\": current_frame,\n",
    "                    \"emotion\": current_emotion\n",
    "                }\n",
    "                emotion_results.append(frame_data)\n",
    "\n",
    "        # Display progress bar\n",
    "        cv2.putText(frame, f'Progress: {progress:.1f}%', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Display\n",
    "        cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "\n",
    "    # Key controls\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q') or not ret:  # Press 'q' to quit\n",
    "        break\n",
    "    elif key == ord(' '):  # Press 'space' to pause/resume\n",
    "        paused = not paused\n",
    "    elif key == ord('s'):  # Press 's' to save current frame\n",
    "        frame_filename = f'frame_{current_frame}.jpg'\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        print(f\"Saved frame to {frame_filename}\")\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save all results to JSON file\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(emotion_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RAG-based Mental Health Report Generation Module\n",
    "\n",
    "This module utilizes retrieval-augmented generation techniques to improve the generation quality of the LLM using mental health literatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the API key for the Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes the RAG system with the provided PDF documents and sets up the RAG pipeline for generating responses based on the emotional patterns detected from the video analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and extract mental health literature from PDF files in a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"docs\"\n",
    "docs = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            reader = PdfReader(filepath)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "            docs.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents([Document(page_content=doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a FAISS vector store from the documents and their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the LLM for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.6, google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Instructions:\n",
    "You are an expert mental health support AI assistant tasked with analyzing emotional patterns and providing personalized guidance. Your role is to:\n",
    "1. Analyze the user's emotional history from facial recognition data\n",
    "2. Cross-reference this information with mental health resources\n",
    "3. Generate a comprehensive yet sensitive support response\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Emotional Pattern Summary:\n",
    "{question}\n",
    "\n",
    "Please provide a response that includes:\n",
    "1. Pattern Analysis:\n",
    "   - Identify predominant emotions and significant emotional shifts\n",
    "   - Note any recurring patterns or triggers\n",
    "   - Highlight unusual or concerning emotional states\n",
    "\n",
    "2. Professional Insights:\n",
    "   - Draw relevant connections to mental health literature from the provided documents\n",
    "   - Explain potential implications of observed emotional patterns\n",
    "   - Identify any patterns that may warrant professional attention\n",
    "\n",
    "3. Personalized Recommendations:\n",
    "   - Suggest evidence-based coping strategies\n",
    "   - Provide actionable steps for emotional regulation\n",
    "   - Recommend relevant self-care practices\n",
    "\n",
    "Important Guidelines:\n",
    "- Maintain a professional yet empathetic tone\n",
    "- Focus on objective observations rather than diagnoses\n",
    "- You can cite specific sources from provided documents when making recommendations if you think they are relevant\n",
    "- Try to make the response short and concise while covering all relevant aspects\n",
    "- Focus on the most significant patterns identified in the summary\n",
    "- Provide practical, personalized recommendations\n",
    "- Some transient emotional states may be a missed detection if they disappear quickly and contradict the overall pattern\n",
    "\n",
    "Note: If the retrieved documents don't directly address the observed patterns, provide general evidence-based guidance while clearly indicating this limitation.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"top_k\": 5}), \n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the detected emotion history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = ''\n",
    "with open(f'{target_file}.json', 'r') as file:\n",
    "    query = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform initial emotion summarization by sending the emotion detection history directly to a lightweight LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure query is a string\n",
    "if isinstance(query, list):\n",
    "    # convert the list of dictionaries to a string by joining the 'emotion' values\n",
    "    query = ' '.join( '[timestamp: ' + item['timestamp'] + ', emotion: ' + item['emotion'] + '].\\n' for item in query if 'emotion' in item)\n",
    "\n",
    "prompt = f'''\n",
    "\n",
    "Please read the following emotion detection history and provide a summary that includes:\n",
    "\n",
    "1. Primary Emotional States:\n",
    "   - Most frequent emotions\n",
    "   - Duration and intensity patterns\n",
    "   - Significant emotional transitions\n",
    "\n",
    "2. Temporal Patterns:\n",
    "   - Daily/weekly cycles if present\n",
    "   - Triggering patterns or sequences\n",
    "   - Unusual or outlier emotional states\n",
    "\n",
    "3. Key Statistics:\n",
    "   - Distribution of different emotions\n",
    "   - Average duration of emotional states\n",
    "   - Frequency of emotional changes\n",
    "\n",
    "Note that you should avoid making diagnoses or providing specific advice. If some emtions are only transient and disappear quickly, you can simply note that they may be missed detections.\n",
    "\n",
    "Raw Emotion Detection Data:\n",
    "{query}\n",
    "'''\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\", temperature=0.7, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "initial_emotion_summarization = llm.invoke(prompt).content\n",
    "print(initial_emotion_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the retrieved documents if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = vector_store.as_retriever().invoke(initial_emotion_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the final mental health report based on the initial emotion summarization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_chain.invoke(initial_emotion_summarization)\n",
    "print(answer['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcnn_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
